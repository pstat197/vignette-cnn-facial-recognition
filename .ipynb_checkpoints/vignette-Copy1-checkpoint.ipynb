{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Convolutional Neural Networks: An Introduction to Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Template for setting up table of contents (part 1)\n",
    "\n",
    "# Table of contents\n",
    "1. [Introduction](#introduction)\n",
    "2. [Some paragraph](#paragraph1)\n",
    "    1. [Sub paragraph](#subparagraph1)\n",
    "3. [Another paragraph](#paragraph2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Template for setting up the table of contents (part 2)\n",
    "\n",
    "## This is the introduction <a name=\"introduction\"></a>\n",
    "Some introduction text, formatted in heading 2 style\n",
    "\n",
    "## Some paragraph <a name=\"paragraph1\"></a>\n",
    "The first paragraph text\n",
    "\n",
    "### Sub paragraph <a name=\"subparagraph1\"></a>\n",
    "This is a sub paragraph, formatted in heading 3 style\n",
    "\n",
    "## Another paragraph <a name=\"paragraph2\"></a>\n",
    "The second paragraph text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the lab sctructure:\n",
    "1. Objective\n",
    "2. Setup\n",
    "    This are all the packages that are required to install\n",
    "    Installaion of certain packages (m2 install)\n",
    "3. Data Selection\n",
    "    Include summary statistics of the meta inforamtion of our data\n",
    "    resolution\n",
    "    distribution of meta data\n",
    "    sumary of how the data was gathered\n",
    "    link to where the data set can be downloaded\n",
    "    what subset of data are we using how did we select it\n",
    "\n",
    "4. Image Augmentation \n",
    "    How to understand how loayers of cnn affect the image\n",
    "        (kernals , filters)\n",
    "    Guassian Blur, Uniform Blur, Edge detection (horizontal/vertical kernals)\n",
    "\n",
    "5. Image Preprocessing\n",
    "    Preparing images for input into a cnn\n",
    "\n",
    "6. CNN for binary face classification\n",
    "    alex net 8\n",
    "    show what each layer is doing \n",
    "    https://www.tensorflow.org/guide/keras/sequential_model#feature_extraction_with_a_sequential_model\n",
    "    \n",
    "\n",
    "7. CNN for multi classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Setup\n",
    "Required Packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Sequential\n",
    "from keras import datasets, layers, models\n",
    "from keras.utils import np_utils\n",
    "from keras import regularizers\n",
    "from keras.layers import Dense, Dropout, BatchNormalization\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Image Augmentation\n",
    "\n",
    "To augment an image we create a kernel, which is just a matrix of values. This matrix is then slid across the input image which results in the changed appearance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to Sharpen Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " image = train_images[5]\n",
    "\n",
    "\n",
    "kernel = np.array([[0, -1, 0], #kernel for sharpening image\n",
    "                   [-1, 5,-1],\n",
    "                   [0, -1, 0]])\n",
    "image_sharp = cv2.filter2D(src=image, ddepth=-1, kernel=kernel)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to Blur Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_blur = cv2.GaussianBlur(image, (3,3), 0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to Preform Edge Detection\n",
    "NOTE: Blurring (smoothing) before edge detection is crucial to remove noise that may cause false edges. The image inputted is not the original image but rather the blurred image from the previous cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = cv2.Canny(image=img_blur, threshold1=100, threshold2=200) # Canny Edge Detection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combing Multiple Filters:  Blurring then Sharpening "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_sharp_with_blur = cv2.filter2D(src=img_blur, ddepth=-1, kernel=kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plotting the Image Modifications \n",
    "plt.subplot(2,3,1)\n",
    "plt.imshow(image)\n",
    "plt.xlabel(\"Orginal Image\")\n",
    "plt.subplot(2,3,2)\n",
    "plt.imshow(image_sharp)\n",
    "plt.xlabel(\"Image Sharpened\")\n",
    "plt.subplot(2,3,3)\n",
    "plt.imshow(img_blur)\n",
    "plt.xlabel(\"Image Blurred\")\n",
    "plt.subplot(2,3,4)\n",
    "plt.imshow(edges)\n",
    "plt.xlabel(\"Edge Detection\")\n",
    "plt.subplot(2,3,5)\n",
    "plt.xlabel(\"Image Blurred + Sharpened\")\n",
    "plt.imshow(image_sharp_with_blur)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) CNN for Multi-Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5d0902",
   "metadata": {},
   "source": [
    "### Split Data Into Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b654d972",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50c44db",
   "metadata": {},
   "source": [
    "### Visualizing the Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the number of rows (records) and columns (features)\n",
    "print(train_images.shape)\n",
    "print(train_labels.shape)\n",
    "print(test_images.shape)\n",
    "print(test_labels.shape)\n",
    "\n",
    "print(np.unique(train_labels))\n",
    "print(np.unique(test_labels))\n",
    "\n",
    "# Class Labels \n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "\n",
    "# Visualizing some of the images from the training dataset\n",
    "\n",
    "plt.figure(figsize=[10,10])\n",
    "for i in range (25):    # for first 25 images\n",
    "  plt.subplot(5, 5, i+1)\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "  plt.grid(False)\n",
    "  plt.imshow(train_images[i], cmap=plt.cm.binary)\n",
    "  plt.xlabel(class_names[train_labels[i][0]])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdc44d9",
   "metadata": {},
   "source": [
    "### Changing the Data to Work with the Model\n",
    "Since images are read in as matrices, where each value is a pixel, we want to standardize the pixels and make sure they are float type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5cefccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing/Normalizing is to convert all pixel values to values between 0 and 1.\n",
    "# converting type to float is that to_categorical (one hot encoding) needs the data to be of type float by default.\n",
    "# using to_categorical is that the loss function that we will be using in this code (categorical_crossentropy) when compiling the model needs data to be one hot encoded.\n",
    "\n",
    "train_images = train_images.astype('float32')\n",
    "test_images = test_images.astype('float32')\n",
    " \n",
    "# Standardizing (255 is the total number of pixels an image can have)\n",
    "train_images = train_images / 255\n",
    "test_images = test_images / 255 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df142d1",
   "metadata": {},
   "source": [
    "### Creating the CNN and Training the Model\n",
    "The essence of CNN:\n",
    "1. Add a convultion layer that detects and extracts multiple patterns/features from the image\n",
    "2. Add a pooling layer that reduces the size of the feature map. We used max pooling which takes the largest element from the feature map created from the Conv2D layer. This prevents overfitting.\n",
    "3. Repeat step 1 and 2 multiple times\n",
    "4. Dropout Layer: drops neurons from the neural network (in this case 50%). This improves the performance of the model and prevents overfitting by simplifying the NN.\n",
    "5. Flatten : The input image is flattend and fed to the Fully Connected Layet (connection from the convolution to the dense layer).\n",
    "6. Dense: Used for the output layer, softmax gives a probability for each class. The model makes it prediction based on the class with highest probability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828484f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-09 00:47:46.023764: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 32, 32, 32)        896       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 32, 32, 32)       128       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 32, 32, 32)        9248      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 32, 32, 32)       128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 16, 16, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 16, 16, 32)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 16, 16, 64)        18496     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 16, 16, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 16, 16, 64)        36928     \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 16, 16, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 8, 8, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 8, 8, 64)          0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 8, 8, 128)         73856     \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 8, 8, 128)        512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 8, 8, 128)         147584    \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 8, 8, 128)        512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 4, 4, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 4, 4, 128)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 2048)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               262272    \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 128)              512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 552,874\n",
      "Trainable params: 551,722\n",
      "Non-trainable params: 1,152\n",
      "_________________________________________________________________\n",
      "Epoch 1/14\n",
      "782/782 [==============================] - 171s 215ms/step - loss: 1.7157 - accuracy: 0.4088 - val_loss: 1.2068 - val_accuracy: 0.5694\n",
      "Epoch 2/14\n",
      "324/782 [===========>..................] - ETA: 1:40 - loss: 1.2443 - accuracy: 0.5546"
     ]
    }
   ],
   "source": [
    "# One hot encoding the target class (labels)\n",
    "num_classes = 10\n",
    "train_labels = np_utils.to_categorical(train_labels, num_classes)\n",
    "test_labels = np_utils.to_categorical(test_labels, num_classes)\n",
    "\n",
    "\n",
    "#CNN Coding and Building the Layers\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(layers.Conv2D(32, (3,3), padding='same', activation='relu', input_shape=(32,32,3)))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Conv2D(32, (3,3), padding='same', activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Conv2D(64, (3,3), padding='same', activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Conv2D(64, (3,3), padding='same', activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Conv2D(128, (3,3), padding='same', activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Conv2D(128, (3,3), padding='same', activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(num_classes, activation='softmax'))    # num_classes = 10\n",
    "\n",
    "# Checking the model summary\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# Optimizer used during Back Propagation for weight and bias adjustment - Adam (adjusts the learning rate adaptively).\n",
    "# Loss Function used - Categorical Crossentropy (used when multiple categories/classes are present).\n",
    "# Metrics used for evaluation - Accuracy.\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss=keras.losses.categorical_crossentropy, metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_images, train_labels, batch_size=64, epochs=14,\n",
    "                    validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bf33f8",
   "metadata": {},
   "source": [
    "### Visualizing the Accuracy of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Curve - Comparing the Training Loss with the Testing Loss over increasing Epochs.\n",
    "# Accuracy Curve - Comparing the Training Accuracy with the Testing Accuracy over increasing Epochs.\n",
    "# Loss curve\n",
    "plt.figure(figsize=[6,4])\n",
    "plt.plot(history.history['loss'], 'black', linewidth=2.0)\n",
    "plt.plot(history.history['val_loss'], 'green', linewidth=2.0)\n",
    "plt.legend(['Training Loss', 'Validation Loss'], fontsize=14)\n",
    "plt.xlabel('Epochs', fontsize=10)\n",
    "plt.ylabel('Loss', fontsize=10)\n",
    "plt.title('Loss Curves', fontsize=12)\n",
    "\n",
    "\n",
    "# Accuracy curve\n",
    "plt.figure(figsize=[6,4])\n",
    "plt.plot(history.history['accuracy'], 'black', linewidth=2.0)\n",
    "plt.plot(history.history['val_accuracy'], 'blue', linewidth=2.0)\n",
    "plt.legend(['Training Accuracy', 'Validation Accuracy'], fontsize=14)\n",
    "plt.xlabel('Epochs', fontsize=10)\n",
    "plt.ylabel('Accuracy', fontsize=10)\n",
    "plt.title('Accuracy Curves', fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61717fa7",
   "metadata": {},
   "source": [
    "### Using the Model and Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the Predictions\n",
    "pred = model.predict(test_images)\n",
    "print(pred)\n",
    "\n",
    "# Converting the predictions into label index \n",
    "pred_classes = np.argmax(pred, axis=1)\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(5, 5, figsize=(15,15))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i in np.arange(0, 25):\n",
    "    axes[i].imshow(test_images[i])\n",
    "    axes[i].set_title(\"True: %s \\nPredict: %s\" % (class_names[np.argmax(test_labels[i])], class_names[pred_classes[i]]))\n",
    "    axes[i].axis('off')\n",
    "    plt.subplots_adjust(wspace=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21124b0",
   "metadata": {},
   "source": [
    "### Functions created for next output visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image(i, predictions_array, true_label, img):\n",
    "    predictions_array, true_label, img = predictions_array, true_label[i], img[i]\n",
    "    plt.grid(False)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    plt.imshow(img, cmap=plt.cm.binary)\n",
    "\n",
    "    predicted_label = np.argmax(predictions_array)\n",
    "    if predicted_label == true_label:\n",
    "        color = 'blue'\n",
    "    else:\n",
    "        color = 'red'\n",
    "\n",
    "    plt.xlabel(f\"{class_names[int(predicted_label)]} {100*np.max(predictions_array):2.0f}% ({class_names[int(true_label)]})\", \n",
    "               color=color)\n",
    "    plt.show()\n",
    "\n",
    "def plot_value_array(i, predictions_array, true_label):\n",
    "    predictions_array, true_label = predictions_array, int(true_label[i])\n",
    "    plt.grid(False)\n",
    "    plt.xticks(range(10))\n",
    "    plt.yticks([])\n",
    "    thisplot = plt.bar(range(10), predictions_array, color=\"#777777\")\n",
    "    plt.ylim([0, 1])\n",
    "    predicted_label = np.argmax(predictions_array)\n",
    "\n",
    "    thisplot[predicted_label].set_color('red')\n",
    "    thisplot[true_label].set_color('blue')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9d3f95",
   "metadata": {},
   "source": [
    "### NEEDS TO BE IMPROVED: Showing the certainty of the predictions made by the model\n",
    "\n",
    "** Can't figure out why subplot isn't putting the images side by side\n",
    "\n",
    "As mentioned before, the softmax layer gives a probability for each class. Here we can visualize what those exact probabilities are! We pulled two images of ships to see how our model interperted them differently! \n",
    "\n",
    "### NOTE: \n",
    "The X-Axis Values correspond to the following 10 classes:\n",
    "\n",
    "0 = airplane,\n",
    "1 = automobile,\n",
    "2 = bird,\n",
    "3 = cat,\n",
    "4 = deer,\n",
    "5 = dog,\n",
    "6 = frog,\n",
    "7 = horse,\n",
    "8 = ship,\n",
    "9 = truck\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = 8\n",
    "num_cols = 5\n",
    "num_images = num_rows * num_cols\n",
    "i = 1\n",
    "plt.subplot(2,2,1)\n",
    "plot_image(i, pred[i], test_labels, test_images)\n",
    "plt.subplot(2,2,2)\n",
    "plot_value_array(i, pred[i], test_labels)\n",
    "i = 2\n",
    "plt.subplot(2,2,3)\n",
    "plot_image(i, pred[i], test_labels, test_images)\n",
    "plt.subplot(2,2,4)\n",
    "plot_value_array(i, pred[i], test_labels)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Same as above but with 40 Images, images are really small. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = 8\n",
    "num_cols = 5\n",
    "num_images = num_rows * num_cols\n",
    "plt.figure(figsize=(4, 4))\n",
    "for i in range(num_images):\n",
    "    plt.subplot(2, 21, i + 1)\n",
    "    plot_image(i, pred[i], test_labels, test_images)\n",
    "    plt.subplot(num_rows, 21, i+2)\n",
    "    plot_value_array(i, pred[i], test_labels)\n",
    "\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "19181e7bf0f08410f65fb3bf80e77c09c5fe1830899a9abdc34573591b362759"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
